{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ref: https://python.langchain.com/docs/tutorials/rag/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the model, and which embeddings and vector stores to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/Snorlax/Projects/cs-minor-6sem/langchain-demo/env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_huggingface.embeddings import HuggingFaceEndpointEmbeddings # we'll locally use huggingface embeddings\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "\n",
    "# https://python.langchain.com/docs/integrations/chat/ollama/\n",
    "llm = ChatOllama(\n",
    "    model=\"llama3.2:3b\",\n",
    "    temperature=0,\n",
    "    # other params...\n",
    ")\n",
    "\n",
    "embeddings = HuggingFaceEndpointEmbeddings()\n",
    "vector_store = InMemoryVectorStore(embeddings)\n",
    "\n",
    "# template = ChatPromptTemplate.from_template(\n",
    "#     \"What happens when an unstoppable force meets an immovable object?\"\n",
    "# )\n",
    "# chain = template | llm\n",
    "\n",
    "# response = chain.invoke({})\n",
    "# print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [OPTIONAL] On kaggle, for local calls, tinyllama is recommended, and otherwise, you may use the 8B llama model as an API call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from dotenv import load_dotenv\n",
    "from langchain_huggingface import ChatHuggingFace, HuggingFacePipeline # to get models\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_huggingface.embeddings import HuggingFaceEndpointEmbeddings # locally use embeddings\n",
    "# embeddings = HuggingFaceEndpointEmbeddings()\n",
    "\n",
    "# load_dotenv()\n",
    "\n",
    "# https://python.langchain.com/docs/integrations/chat/huggingface/#huggingfacepipeline\n",
    "llm = HuggingFacePipeline.from_model_id(\n",
    "    model_id=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
    "    task=\"text-generation\",\n",
    "    pipeline_kwargs=dict(\n",
    "        max_new_tokens=512,\n",
    "        do_sample=False,\n",
    "        repetition_penalty=1.03,\n",
    "    ),\n",
    ")\n",
    "\n",
    "llm = ChatHuggingFace(llm=llm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up indexing\n",
    "\n",
    "Indexing is basically how you organise your data for later use. This involves gathering it first, and then converting that to embeddings, and storing that into a vector store DB, so we are able to efficiently index it later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total characters: 43130\n",
      "Split blog post into 66 sub-documents.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/Snorlax/Projects/cs-minor-6sem/langchain-demo/env/lib/python3.11/site-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a8140ad2-bb83-4072-84a1-e4c55109de6c', '22f70386-7c12-4ea2-a888-2c04d6e45ac4', 'f3512aef-4192-4fee-a0b9-4cefb08a6bc5']\n"
     ]
    }
   ],
   "source": [
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "\n",
    "# 1. get the data\n",
    "\n",
    "# Only keep post title, headers, and content from the full HTML.\n",
    "bs4_strainer = bs4.SoupStrainer(class_=(\"post-title\", \"post-header\", \"post-content\"))\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs={\"parse_only\": bs4_strainer},\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "assert len(docs) == 1 # makes sure docs contains only one document\n",
    "print(f\"Total characters: {len(docs[0].page_content)}\")\n",
    "\n",
    "\n",
    "# 2. split the document into more manageable chunks\n",
    "\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,  # chunk size (characters)\n",
    "    chunk_overlap=200,  # chunk overlap (characters)\n",
    "    add_start_index=True,  # track index in original document\n",
    ")\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "print(f\"Split blog post into {len(all_splits)} sub-documents.\")\n",
    "\n",
    "\n",
    "# 3. store it into a vector store\n",
    "\n",
    "document_ids = vector_store.add_documents(documents=all_splits)\n",
    "print(document_ids[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The fun part: Information retrieval and generation\n",
    "\n",
    "Our retrieval step simply runs a similarity search using the input question, and the generation step formats the retrieved context and original question into a prompt for the chat model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Modular Reasoning (MRKL) is a neuro-symbolic architecture for autonomous agents that uses a collection of \"expert\" modules, which can be neural or symbolic, to accomplish tasks. These modules are connected by a general-purpose Large Language Model (LLM) that routes inquiries to the best suitable expert module. The goal is to leverage the strengths of both symbolic and neural approaches to improve efficiency and accuracy in complex tasks.' additional_kwargs={} response_metadata={'model': 'llama3.2:3b', 'created_at': '2025-03-16T11:49:18.076984622Z', 'done': True, 'done_reason': 'stop', 'total_duration': 7140794736, 'load_duration': 2062254912, 'prompt_eval_count': 665, 'prompt_eval_duration': 2909000000, 'eval_count': 86, 'eval_duration': 2168000000, 'message': Message(role='assistant', content='', images=None, tool_calls=None)} id='run-47ab1585-a602-4c50-a04a-68ef0ca10055-0' usage_metadata={'input_tokens': 665, 'output_tokens': 86, 'total_tokens': 751}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
    "Question: {question} \n",
    "Context: {context} \n",
    "Answer:\"\"\"\n",
    ")\n",
    "\n",
    "question = \"...\"\n",
    "\n",
    "retrieved_docs = vector_store.similarity_search(question)                   # retrieve\n",
    "docs_content = \"\\n\\n\".join(doc.page_content for doc in retrieved_docs)      # generate\n",
    "\n",
    "chain = prompt | llm\n",
    "answer = chain.invoke({\"question\": \"What is Modular Reasoning?\", \"context\": docs_content})\n",
    "\n",
    "print(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
