{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ref: https://python.langchain.com/docs/tutorials/rag/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **[LOCAL]** Set up the model, and which embeddings and vector stores to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings # local\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# https://python.langchain.com/docs/integrations/chat/ollama/\n",
    "llm = ChatOllama(\n",
    "    model=\"llama3.2:3b\",\n",
    "    temperature=0,\n",
    "    # other params...\n",
    ")\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
    "vector_store = InMemoryVectorStore(embeddings)\n",
    "\n",
    "# template = ChatPromptTemplate.from_template(\n",
    "#     \"What happens when an unstoppable force meets an immovable object?\"\n",
    "# )\n",
    "# chain = template | llm\n",
    "\n",
    "# response = chain.invoke({})\n",
    "# print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### **[API CALLS]** On kaggle, for local calls, tinyllama is recommended, and otherwise, you may use the 3B llama model as an API call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install accelerate \\\n",
    "            beautifulsoup4 \\\n",
    "            huggingface_hub \\\n",
    "            langchain \\\n",
    "            langchain-community \\\n",
    "            langchain-huggingface \\\n",
    "            python-dotenv \\\n",
    "            requests \\\n",
    "            sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kaggle_secrets import UserSecretsClient\n",
    "import os\n",
    "\n",
    "user_secrets = UserSecretsClient()\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = user_secrets.get_secret(\"HUGGINGFACEHUB_API_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_huggingface.embeddings import HuggingFaceEndpointEmbeddings # calls the api\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# https://python.langchain.com/docs/integrations/chat/huggingface/#huggingfacepipeline\n",
    "from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint\n",
    "\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=\"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "    task=\"text-generation\",\n",
    "    temperature=0,\n",
    "    # max_new_tokens=512,\n",
    "    # do_sample=False,\n",
    "    # repetition_penalty=1.03,\n",
    ")\n",
    "\n",
    "llm = ChatHuggingFace(llm=llm)\n",
    "embeddings = HuggingFaceEndpointEmbeddings()\n",
    "vector_store = InMemoryVectorStore(embeddings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up indexing\n",
    "\n",
    "Indexing is basically how you organise your data for later use. This involves gathering it first, and then converting that to embeddings, and storing that into a vector store DB, so we are able to efficiently index it later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "\n",
    "# 1. get the data\n",
    "\n",
    "# Only keep post title, headers, and content from the full HTML.\n",
    "bs4_strainer = bs4.SoupStrainer(class_=(\"post-title\", \"post-header\", \"post-content\"))\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs={\"parse_only\": bs4_strainer},\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "# import pickle\n",
    "# # ## store\n",
    "# # with open(\"assets/loaded-docs.bin\", \"wb\") as f:\n",
    "# #     pickle.dump(docs, f)\n",
    "\n",
    "# ## load\n",
    "# with open(\"assets/loaded-docs.bin\", \"rb\") as f:\n",
    "#     docs = pickle.load(f)\n",
    "\n",
    "\n",
    "assert len(docs) == 1 # makes sure docs contains only one document\n",
    "print(f\"Total characters: {len(docs[0].page_content)}\")\n",
    "\n",
    "\n",
    "# 2. split the document into more manageable chunks\n",
    "\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,  # chunk size (characters)\n",
    "    chunk_overlap=200,  # chunk overlap (characters)\n",
    "    add_start_index=True,  # track index in original document\n",
    ")\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "print(f\"Split blog post into {len(all_splits)} sub-documents.\")\n",
    "\n",
    "\n",
    "# 3. store it into a vector store\n",
    "\n",
    "document_ids = vector_store.add_documents(documents=all_splits)\n",
    "print(document_ids[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The fun part: Information retrieval and generation\n",
    "\n",
    "Our retrieval step simply runs a similarity search using the input question, and the generation step formats the retrieved context and original question into a prompt for the chat model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
    "Question: {question} \n",
    "Context: {context} \n",
    "Answer:\"\"\"\n",
    ")\n",
    "\n",
    "# question = \"what is your opinion on pineapple pizza\"\n",
    "question = \"What is Modular Reasoning?\"\n",
    "\n",
    "retrieved_docs = vector_store.similarity_search(question)                   # retrieve\n",
    "docs_content = \"\\n\\n\".join(doc.page_content for doc in retrieved_docs)      # generate\n",
    "\n",
    "chain = prompt | llm\n",
    "answer = chain.invoke({\"question\": question, \"context\": docs_content})\n",
    "\n",
    "print(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
